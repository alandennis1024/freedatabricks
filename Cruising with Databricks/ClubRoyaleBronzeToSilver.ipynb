{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ccba8a-59ce-4f9f-ada1-332fc61c8e38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Class Definition"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DeltaTableIncrementalMerger:\n",
    "    \"\"\"\n",
    "    A class to incrementally merge records from a source Delta table into a target Delta table.\n",
    "    It creates the target table from the source table schema if it does not exist.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_table: str,\n",
    "        target_table: str,\n",
    "        checkpoint_location: str,\n",
    "        order_by_col: str = \"DateIngested\",\n",
    "        merge_condition: str = \"target_df.RewardNumber = batch_df.RewardNumber AND target_df.OfferCode = batch_df.OfferCode\",\n",
    "        partition_by_cols: list = [\"RewardNumber\", \"OfferCode\"],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DeltaTableIncrementalMerger with the given parameters.\n",
    "\n",
    "        :param source_table: The source Delta table name.\n",
    "        :param target_table: The target Delta table name.\n",
    "        :param checkpoint_location: The location for storing checkpoints.\n",
    "        :param order_by_col: The column to order by for incremental merging.\n",
    "        :param merge_condition: The condition for merging records.\n",
    "        :param partition_by_cols: The columns to partition by.\n",
    "        \"\"\"\n",
    "        self.source_table = source_table\n",
    "        self.target_table = target_table\n",
    "        self.checkpoint_location = checkpoint_location\n",
    "        self.order_by_col = order_by_col\n",
    "        self.merge_condition = merge_condition\n",
    "        self.partition_by_cols = partition_by_cols\n",
    "\n",
    "    def create_table_from_template_if_not_exists(self, cols_to_remove: list = []):\n",
    "        \"\"\"\n",
    "        Create the target table from the source table schema if it does not exist.\n",
    "\n",
    "        :param cols_to_remove: List of columns to remove from the source schema.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Entering create_table_from_template_if_not_exists\")\n",
    "        if not spark.catalog.tableExists(self.target_table):\n",
    "            template_df = spark.table(self.source_table)\n",
    "            cols = [c for c in template_df.columns if c not in cols_to_remove]\n",
    "            schema_str = \", \".join(\n",
    "                f\"{field.name} {field.dataType.simpleString()}\"\n",
    "                for field in template_df.schema.fields\n",
    "                if field.name in cols\n",
    "            )\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                CREATE TABLE {self.target_table} ({schema_str})\n",
    "                USING DELTA\n",
    "                TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "                CLUSTER BY AUTO\n",
    "                \"\"\"\n",
    "            )\n",
    "        logger.debug(\"Exiting create_table_from_template_if_not_exists\")\n",
    "\n",
    "    def start_incremental_merge(self):\n",
    "        \"\"\"\n",
    "        Start the incremental merge process from the source table to the target table.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Entering start_incremental_merge\")\n",
    "        self.create_table_from_template_if_not_exists()\n",
    "\n",
    "        # Read the change data feed from the source table\n",
    "        source_cdf_df = (\n",
    "            spark.readStream.format(\"delta\")\n",
    "            .option(\"readChangeData\", \"true\")\n",
    "            .table(self.source_table)\n",
    "            .filter(\"_change_type = 'insert'\")\n",
    "        )\n",
    "\n",
    "        def foreach_batch_function(batch_df, batch_id):\n",
    "            \"\"\"\n",
    "            Function to process each batch of data during the streaming read.\n",
    "            :param batch_df: The DataFrame for the current batch.\n",
    "            :param batch_id: The ID of the current batch.\n",
    "            \"\"\"\n",
    "            logger.debug(f\"Processing batch {batch_id}\")\n",
    "            window_spec = Window.partitionBy(*self.partition_by_cols).orderBy(\n",
    "                F.col(self.order_by_col).desc()\n",
    "            )\n",
    "            batch_df = (\n",
    "                batch_df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "                .filter(F.col(\"row_num\") == 1)\n",
    "                .drop(\"row_num\")\n",
    "            )\n",
    "\n",
    "            target_df = spark.table(self.target_table)\n",
    "            batch_df.createOrReplaceTempView(\"batch_df\")\n",
    "            target_df.createOrReplaceTempView(\"target_df\")\n",
    "\n",
    "            # Perform the merge operation\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                MERGE INTO {self.target_table} AS target_df\n",
    "                USING batch_df\n",
    "                ON {self.merge_condition}\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "                WHEN NOT MATCHED BY SOURCE THEN DELETE\n",
    "                \"\"\"\n",
    "            )\n",
    "            logger.debug(f\"Completed processing batch {batch_id}\")\n",
    "\n",
    "        # Start the streaming query with the foreachBatch function\n",
    "        streaming_query = (\n",
    "            source_cdf_df.writeStream.foreachBatch(foreach_batch_function)\n",
    "            .option(\"checkpointLocation\", self.checkpoint_location)\n",
    "            .trigger(availableNow=True)\n",
    "            .start()\n",
    "        )\n",
    "        streaming_query.awaitTermination()\n",
    "        logger.debug(\"Exiting start_incremental_merge\")\n",
    "\n",
    "\n",
    "# Initialize the merger and start the incremental merge process\n",
    "merger = DeltaTableIncrementalMerger(\n",
    "    source_table=\"clubroyale.offers.bronze_offers\",\n",
    "    target_table=\"clubroyale.offers.silver_offers\",\n",
    "    checkpoint_location=\"/Volumes/clubroyale/offers/checkpoints/bronze_to_silver\",\n",
    "    order_by_col=\"DateIngested\",\n",
    "    partition_by_cols=[\"RewardNumber\", \"OfferCode\"],\n",
    ")\n",
    "merger.start_incremental_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1dd9090-2c85-40ab-a5a1-1241dd788bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select count(*) from clubroyale.offers.bronze_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ab8ea7b-2c07-4474-ac18-8ba08c681c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6106915087671021,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ClubRoyaleBronzeToSilver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
