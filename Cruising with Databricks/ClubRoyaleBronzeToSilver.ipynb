{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ccba8a-59ce-4f9f-ada1-332fc61c8e38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Class Definition"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "class DeltaTableIncrementalMerger:\n",
    "    \"\"\"\n",
    "    A class to incrementally merge records from a source Delta table into a target Delta table.\n",
    "    It creates the target table from the source table schema if it does not exist.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_table: str,\n",
    "        target_table: str,\n",
    "        checkpoint_location: str,\n",
    "        order_by_col: str = \"DateIngested\",\n",
    "        merge_condition: str = \"target_df.RewardNumber = batch_df.RewardNumber AND target_df.OfferCode = batch_df.OfferCode\",\n",
    "        partition_by_cols: list = [\"RewardNumber\", \"OfferCode\"]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DeltaTableIncrementalMerger with the given parameters.\n",
    "\n",
    "        :param source_table: The source Delta table name.\n",
    "        :param target_table: The target Delta table name.\n",
    "        :param checkpoint_location: The location for storing checkpoints.\n",
    "        :param order_by_col: The column to order by for incremental merging.\n",
    "        :param merge_condition: The condition for merging records.\n",
    "        :param partition_by_cols: The columns to partition by.\n",
    "        \"\"\"\n",
    "        self.source_table = source_table\n",
    "        self.target_table = target_table\n",
    "        self.checkpoint_location = checkpoint_location\n",
    "        self.order_by_col = order_by_col\n",
    "        self.merge_condition = merge_condition\n",
    "        self.partition_by_cols = partition_by_cols\n",
    "\n",
    "    def create_table_from_template_if_not_exists(self, cols_to_remove: list = []):\n",
    "        \"\"\"\n",
    "        Create the target table from the source table schema if it does not exist.\n",
    "\n",
    "        :param cols_to_remove: List of columns to remove from the source schema.\n",
    "        \"\"\"\n",
    "        if not spark.catalog.tableExists(self.target_table):\n",
    "            template_df = spark.table(self.source_table)\n",
    "            cols = [c for c in template_df.columns if c not in cols_to_remove]\n",
    "            schema_str = \", \".join(\n",
    "                f\"{field.name} {field.dataType.simpleString()}\"\n",
    "                for field in template_df.schema.fields\n",
    "                if field.name in cols\n",
    "            )\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                CREATE TABLE {self.target_table} ({schema_str})\n",
    "                USING DELTA\n",
    "                TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "                CLUSTER BY AUTO\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "    def start_incremental_merge(self):\n",
    "        \"\"\"\n",
    "        Start the incremental merge process from the source table to the target table.\n",
    "        \"\"\"\n",
    "        self.create_table_from_template_if_not_exists()\n",
    "\n",
    "        source_cdf_df = (\n",
    "            spark.readStream.format(\"delta\")\n",
    "            .option(\"readChangeData\", \"true\")\n",
    "            .table(self.source_table)\n",
    "            .filter(\"_change_type = 'insert'\")\n",
    "        )\n",
    "\n",
    "        def foreach_batch_function(batch_df, batch_id):\n",
    "            \"\"\"\n",
    "            Function to process each batch of data during the streaming read.\n",
    "\n",
    "            :param batch_df: The DataFrame for the current batch.\n",
    "            :param batch_id: The ID of the current batch.\n",
    "            \"\"\"\n",
    "            window_spec = Window.partitionBy(*self.partition_by_cols).orderBy(\n",
    "                F.col(self.order_by_col).desc()\n",
    "            )\n",
    "            batch_df = (\n",
    "                batch_df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "                .filter(F.col(\"row_num\") == 1)\n",
    "                .drop(\"row_num\")\n",
    "            )\n",
    "\n",
    "            target_df = spark.table(self.target_table)\n",
    "            batch_df.createOrReplaceTempView(\"batch_df\")\n",
    "            target_df.createOrReplaceTempView(\"target_df\")\n",
    "\n",
    "            spark.sql(\n",
    "                f\"\"\"\n",
    "                MERGE INTO {self.target_table} AS target_df\n",
    "                USING batch_df\n",
    "                ON {self.merge_condition}\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        source_cdf_df.writeStream.foreachBatch(foreach_batch_function).option(\n",
    "            \"checkpointLocation\", self.checkpoint_location\n",
    "        ).trigger(availableNow=True).start()\n",
    "\n",
    "        # Select the most recent set of records in self.source_table based on self.order_by_col\n",
    "        most_recent_records_df = (\n",
    "            spark.table(self.source_table)\n",
    "            .withColumn(\"row_num\", F.row_number().over(Window.partitionBy(*self.partition_by_cols).orderBy(F.col(self.order_by_col).desc())))\n",
    "            .filter(F.col(\"row_num\") == 1)\n",
    "            .drop(\"row_num\")\n",
    "        )\n",
    "\n",
    "        target_df = spark.table(self.target_table)\n",
    "        temp_key_col = \"temp_key\"\n",
    "        most_recent_records_df = most_recent_records_df.withColumn(temp_key_col, F.concat_ws(\"_\", *self.partition_by_cols))\n",
    "        target_df = target_df.withColumn(temp_key_col, F.concat_ws(\"_\", *self.partition_by_cols))\n",
    "        most_recent_records_df.createOrReplaceTempView(\"most_recent_records_df\")\n",
    "        target_df.createOrReplaceTempView(\"target_df\")\n",
    "        \n",
    "        # Find records in target_df that are not in the most recent set of records\n",
    "        records_not_in_most_recent_df = target_df.join(\n",
    "            most_recent_records_df, on=[temp_key_col], how=\"left_anti\"\n",
    "        )\n",
    "        records_not_in_most_recent_df.createOrReplaceTempView(\"records_not_in_most_recent_df\")\n",
    "\n",
    "        delete_condition = \" AND \".join([f\"target_df.{col} = records_not_in_most_recent_df.{col}\" for col in self.partition_by_cols])\n",
    "        spark.sql(\n",
    "            f\"\"\"\n",
    "            DELETE FROM {self.target_table} as target_df\n",
    "            WHERE EXISTS (\n",
    "                SELECT 1\n",
    "                FROM records_not_in_most_recent_df\n",
    "                WHERE {delete_condition}\n",
    "            )\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "merger = DeltaTableIncrementalMerger(\n",
    "    source_table=\"clubroyale.offers.bronze_offers\",\n",
    "    target_table=\"clubroyale.offers.silver_offers\",\n",
    "    checkpoint_location=\"/Volumes/clubroyale/offers/checkpoints//bronze_to_silver\",\n",
    "    order_by_col=\"DateIngested\",\n",
    "    partition_by_cols=[\"RewardNumber\", \"OfferCode\"]\n",
    ")\n",
    "merger.start_incremental_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1dd9090-2c85-40ab-a5a1-1241dd788bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select count(*) from clubroyale.offers.bronze_offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e29e53-6242-48de-b12f-0fa7360bb6ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM `clubroyale`.`offers`.`silver_offers`;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6106915087671022,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ClubRoyaleBronzeToSilver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
